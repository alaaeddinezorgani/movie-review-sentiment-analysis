{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7925dd-9e46-43f2-b545-6d88f9b53cde",
   "metadata": {},
   "source": [
    "# importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adedb5e0-3b00-44fa-9836-bb3593013f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #for data manipulation\n",
    "import numpy as np #for data analysis\n",
    "#for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#visualization of frequent words\n",
    "from wordcloud import WordCloud\n",
    "#natural language processing (NLP)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re #cleaning text\n",
    "#ML tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer# vectorization\n",
    "from sklearn.naive_bayes import MultinomialNB#naive bayes for text classification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b6ccb-d5ea-4c49-ba21-08ce38b5d767",
   "metadata": {},
   "source": [
    "# loading Dataset ,displaying head lines (first 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c52f29-b6ef-4f29-aa61-f49cba442ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c3fff1-ec3d-42e7-946c-95d38839ea6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #dataset structure 2 columns 50000 lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b1b9c3-5839-48fe-8c1b-a14c5f3f25ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alaaeddine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/alaaeddine/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/alaaeddine/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef2c3b4-fb22-49d7-aee5-87a726f3e492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:10])#English stopwords \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9872e7-43e9-4541-b1af-cd4419dba980",
   "metadata": {},
   "source": [
    "# text cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f761da-4317-4678-992a-6fe064b1679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>one reviewer mentioned watching oz episode hoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  one reviewer mentioned watching oz episode hoo...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter mattei love time money visually stunnin...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#lemmatizer creation ( grouping many forms of a word in one word )\n",
    "#used in advanced research znd chatboxes\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#function to clean text\n",
    "def clean_text(text):\n",
    "    #removing html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #only letters\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    #Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    #words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "#clean the review column\n",
    "df['clean_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "#example\n",
    "df[['review', 'clean_review']].head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "928d8c41-0482-4178-87d3-aa557dd53fcf",
   "metadata": {},
   "source": [
    "displaying more lines head (10) instead of head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05561865-d2f8-40a1-b395-1f91c4dc0bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>one reviewer mentioned watching oz episode hoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>probably time favorite movie story selflessnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>sure would like see resurrection dated seahunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>show amazing fresh innovative idea first aired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>encouraged positive comment film looking forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>like original gut wrenching laughter like movi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "5  Probably my all-time favorite movie, a story o...   \n",
       "6  I sure would like to see a resurrection of a u...   \n",
       "7  This show was an amazing, fresh & innovative i...   \n",
       "8  Encouraged by the positive comments about this...   \n",
       "9  If you like original gut wrenching laughter yo...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  one reviewer mentioned watching oz episode hoo...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter mattei love time money visually stunnin...  \n",
       "5  probably time favorite movie story selflessnes...  \n",
       "6  sure would like see resurrection dated seahunt...  \n",
       "7  show amazing fresh innovative idea first aired...  \n",
       "8  encouraged positive comment film looking forwa...  \n",
       "9  like original gut wrenching laughter like movi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    #removing html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #only letters\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    #Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    #words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "#clean the review column\n",
    "df['clean_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "#example\n",
    "df[['review', 'clean_review']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5216b95-6c1c-4b9e-b7ef-ee7ee0cd9026",
   "metadata": {},
   "source": [
    "# vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fabb44c1-3a06-4833-a0eb-afbdf645c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (40000, 5000)\n",
      "Testing shape: (10000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vectorization TF IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  #5000 most frequent words\n",
    "X = vectorizer.fit_transform(df['clean_review']).toarray()#transform clean_review to an array\n",
    "\n",
    "#target variable (pos/neg)\n",
    "y = df['sentiment']\n",
    "\n",
    "#dividing datasets into 2 subsets train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training shape:\", X_train.shape)\n",
    "print(\"Testing shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba666f-e740-40c2-bc94-5fc1ed60b4a5",
   "metadata": {},
   "source": [
    "# training the model ,prediction and performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97808f0-75f4-4eb6-ab61-69c0e3445622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8554\n",
      "\n",
      " Confusion Matrix:\n",
      " [[4206  755]\n",
      " [ 691 4348]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.85      0.85      4961\n",
      "    positive       0.85      0.86      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#creation and training model \n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "#performance evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500b11f-c629-46bb-a37b-200ea8a636cd",
   "metadata": {},
   "source": [
    "## ANN - Alaa's part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff04607-1a72-483c-bbf9-ffd2ac17ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f745e80-6cc7-4d00-b1b6-847c82329eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc  = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae3233d2-1f7a-4327-8cc6-babc349b09d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alaaeddine/ws/uni-env/lib/python3.13/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ann_model = Sequential([\n",
    "    Dense(256, activation=\"leaky_relu\", input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation=\"leaky_relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "659bfc48-a7cd-4840-9962-fd642d97a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16460bad-c7df-4794-936c-575efdf5292e",
   "metadata": {},
   "source": [
    "**training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59282b3c-4a93-41c5-80d6-11f6fab9e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7154 - loss: 0.6647 - val_accuracy: 0.8515 - val_loss: 0.5822\n",
      "Epoch 2/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.8562 - loss: 0.4514 - val_accuracy: 0.8823 - val_loss: 0.3335\n",
      "Epoch 3/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.8852 - loss: 0.3025 - val_accuracy: 0.8903 - val_loss: 0.2740\n",
      "Epoch 4/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.8987 - loss: 0.2578 - val_accuracy: 0.8930 - val_loss: 0.2601\n",
      "Epoch 5/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9066 - loss: 0.2386 - val_accuracy: 0.8900 - val_loss: 0.2576\n",
      "Epoch 6/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9126 - loss: 0.2243 - val_accuracy: 0.8880 - val_loss: 0.2579\n",
      "Epoch 7/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9160 - loss: 0.2150 - val_accuracy: 0.8875 - val_loss: 0.2596\n",
      "Epoch 8/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9207 - loss: 0.2062 - val_accuracy: 0.8870 - val_loss: 0.2630\n",
      "Epoch 9/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9222 - loss: 0.2011 - val_accuracy: 0.8865 - val_loss: 0.2653\n",
      "Epoch 10/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9262 - loss: 0.1947 - val_accuracy: 0.8865 - val_loss: 0.2704\n",
      "Epoch 11/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9272 - loss: 0.1904 - val_accuracy: 0.8845 - val_loss: 0.2736\n",
      "Epoch 12/12\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9309 - loss: 0.1832 - val_accuracy: 0.8835 - val_loss: 0.2772\n"
     ]
    }
   ],
   "source": [
    "history = ann_model.fit(\n",
    "    X_train,\n",
    "    y_train_enc,\n",
    "    epochs=12,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8248090-9cf2-49b8-82be-66e20622e410",
   "metadata": {},
   "source": [
    "**evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3154359d-f02d-4840-8d77-4f1a9cdcc1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "=== ANN Results ===\n",
      "Accuracy: 0.8869\n",
      "Confusion Matrix:\n",
      " [[4336  625]\n",
      " [ 506 4533]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.895     0.874     0.885      4961\n",
      "           1      0.879     0.900     0.889      5039\n",
      "\n",
      "    accuracy                          0.887     10000\n",
      "   macro avg      0.887     0.887     0.887     10000\n",
      "weighted avg      0.887     0.887     0.887     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_prob = ann_model.predict(X_test).ravel()\n",
    "y_pred_ann = (y_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== ANN Results ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_enc, y_pred_ann))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_enc, y_pred_ann))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_enc, y_pred_ann, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599fb1ac-e40f-4c0d-b633-0824f35fb0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
